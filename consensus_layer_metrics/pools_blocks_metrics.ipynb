{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "\"\"\"\n",
    "This script obtains the ordered list of slots and which pool was assigned to propose a block\n",
    "Also, it extracts if the slot was missed and how many consecutive slots a pool got (both scheduled or proposed)\n",
    "\"\"\"\n",
    "\n",
    "# Connect to database\n",
    "# %load_ext sql\n",
    "engine = sa.create_engine('postgresql://usr:pwd@localhost:5432/database')\n",
    "\n",
    "%reload_ext sql\n",
    "\n",
    "%sql $engine.url\n",
    "\n",
    "merge_epoch = 146875\n",
    "\n",
    "# 1. Obtain missed blocks\n",
    "sql_stmnt = f\"\"\"\n",
    "    select \n",
    "    f_missed_blocks, \n",
    "    CASE \n",
    "    WHEN f_epoch <= {merge_epoch} THEN 'pre-merge' \n",
    "    WHEN f_epoch > {merge_epoch} THEN 'post-merge' \n",
    "    ELSE NULL END AS f_merge \n",
    "    from t_epoch_metrics_summary \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df_missed_blocks = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# Parse missed blocks to int\n",
    "missed_blocks_list = []\n",
    "for i, row in df_missed_blocks.iterrows():\n",
    "    for j in str(row['f_missed_blocks']).split(\",\"):\n",
    "        text = j.replace(\"[\", \"\")\n",
    "        if text.isdigit():\n",
    "            missed_blocks_list.append(int(text))\n",
    "\n",
    "\n",
    "# Pools pre merge\n",
    "# 2. Obtain scheduled blocks for pools\n",
    "sql_stmnt = f\"\"\"\n",
    "    select t_proposer_duties.f_val_idx, f_pool, f_pool_name, f_proposer_slot, f_proposer_slot/32 as f_epoch\n",
    "    from t_proposer_duties\n",
    "    inner join eth2_pubkeys\n",
    "    on t_proposer_duties.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    where f_proposer_slot/32 <= {merge_epoch}\n",
    "    order by f_proposer_slot asc\n",
    "\"\"\"\n",
    "\n",
    "df_pool_scheduled_pre = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# 3. Obtain proposed blocks for pools\n",
    "sql_stmnt = f\"\"\"\n",
    "    select f_pool, f_proposer_slot, f_proposer_slot as f_proposed_block\n",
    "    from t_proposer_duties\n",
    "    inner join eth2_pubkeys\n",
    "    on t_proposer_duties.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    where f_proposer_slot/32 <= {merge_epoch} and f_proposer_slot not in ({\", \".join(str(x) for x in missed_blocks_list)})\n",
    "    order by f_proposer_slot asc\n",
    "\"\"\"\n",
    "\n",
    "df_pool_proposed_pre = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# 4. Obtain missed blocks for pools\n",
    "sql_stmnt = f\"\"\"\n",
    "    select f_pool, f_proposer_slot, f_proposer_slot as f_missed_block\n",
    "    from t_proposer_duties\n",
    "    inner join eth2_pubkeys\n",
    "    on t_proposer_duties.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    where f_proposer_slot/32 <= {merge_epoch} and f_proposer_slot in ({\", \".join(str(x) for x in missed_blocks_list)})\n",
    "    order by f_proposer_slot asc\n",
    "\"\"\"\n",
    "\n",
    "df_pool_missed_pre = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# Pools post merge\n",
    "# 5. Obtain scheuuled blocks for pools\n",
    "sql_stmnt = f\"\"\"\n",
    "    select t_proposer_duties.f_val_idx, f_pool, f_pool_name, f_proposer_slot, f_proposer_slot/32 as f_epoch\n",
    "    from t_proposer_duties\n",
    "    inner join eth2_pubkeys\n",
    "    on t_proposer_duties.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    where f_proposer_slot/32 > {merge_epoch}\n",
    "    order by f_proposer_slot asc\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df_pool_scheduled_post = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# 6. Obtain proposed blocks for pools\n",
    "sql_stmnt = f\"\"\"\n",
    "    select f_pool, f_proposer_slot, f_proposer_slot as f_proposed_block\n",
    "    from t_proposer_duties\n",
    "    inner join eth2_pubkeys\n",
    "    on t_proposer_duties.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    where f_proposer_slot/32 > {merge_epoch} and f_proposer_slot not in ({\", \".join(str(x) for x in missed_blocks_list)})\n",
    "    order by f_proposer_slot asc\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df_pool_proposed_post = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# 7. Obtain missed blocks for pools\n",
    "sql_stmnt = f\"\"\"\n",
    "    select f_pool, f_proposer_slot, f_proposer_slot as f_missed_block\n",
    "    from t_proposer_duties\n",
    "    inner join eth2_pubkeys\n",
    "    on t_proposer_duties.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    where f_proposer_slot/32 > {merge_epoch} and f_proposer_slot in ({\", \".join(str(x) for x in missed_blocks_list)})\n",
    "    order by f_proposer_slot asc\n",
    "\"\"\"\n",
    "\n",
    "df_pool_missed_post = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "\n",
    "\n",
    "# 8. Obtain all pools at the merge epoch\n",
    "\n",
    "sql_stmnt = f\"\"\"\n",
    "    select distinct(f_pool)\n",
    "    from t_validator_rewards_summary\n",
    "    inner join eth2_pubkeys\n",
    "    on t_validator_rewards_summary.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    where f_epoch = 146875\n",
    "\"\"\"\n",
    "\n",
    "df_pools_pre = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# 8. Obtain all pools at the last post merge epoch we are considering\n",
    "sql_stmnt = f\"\"\"\n",
    "    select distinct(f_pool)\n",
    "    from t_validator_rewards_summary\n",
    "    inner join eth2_pubkeys\n",
    "    on t_validator_rewards_summary.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    where f_epoch = 153875\n",
    "\"\"\"\n",
    "\n",
    "df_pools_post = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# --------------------------- Start analysis ----------------------------------\n",
    "\n",
    "df_pools_pre.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Merge all datasets using the common column: f_pool\n",
    "df_pool_stats_pre = pd.merge(df_pools_pre, df_pool_scheduled_pre, on=['f_pool'], how='outer').fillna(0)\n",
    "df_pool_stats_pre = pd.merge(df_pool_stats_pre, df_pool_proposed_pre, on=['f_pool', 'f_proposer_slot'], how='outer').fillna(0)\n",
    "df_pool_stats_pre = pd.merge(df_pool_stats_pre, df_pool_missed_pre, on=['f_pool', 'f_proposer_slot'], how='outer').fillna(0)\n",
    "df_pool_stats_pre['f_merge'] = 'pre-merge' # hardcode: all the above datasets are pre-merge\n",
    "df_pool_stats_pre = df_pool_stats_pre.sort_values(by=['f_proposer_slot'], ascending=True)\n",
    "\n",
    "\n",
    "df_pool_stats_post = pd.merge(df_pools_post, df_pool_scheduled_post, on=['f_pool'], how='outer').fillna(0)\n",
    "df_pool_stats_post = pd.merge(df_pool_stats_post, df_pool_proposed_post, on=['f_pool', 'f_proposer_slot'], how='outer').fillna(0)\n",
    "df_pool_stats_post = pd.merge(df_pool_stats_post, df_pool_missed_post, on=['f_pool', 'f_proposer_slot'], how='outer').fillna(0)\n",
    "df_pool_stats_post['f_merge'] = 'post-merge'# hardcode: all the above datasets are post-merge\n",
    "df_pool_stats_post = df_pool_stats_post.sort_values(by=['f_proposer_slot'], ascending=True)\n",
    "\n",
    "df_pool_stats = pd.concat([\n",
    "    df_pool_stats_pre, \n",
    "    df_pool_stats_post])\n",
    "\n",
    "\n",
    "# Measure consecutive blocks by pool\n",
    "df_consecutive_stats = df_pool_stats\n",
    "df_consecutive_stats = df_consecutive_stats.set_index('f_proposer_slot') # use proposer_slot as index, common for everyone\n",
    "\n",
    "# this new column will contain \"missed\" if the block was missed\n",
    "df_consecutive_stats['f_pool_missed'] = df_consecutive_stats[['f_pool']]\n",
    "i = 0\n",
    "for index, row in df_consecutive_stats.iterrows():\n",
    "    print(\"Loop line: \", i)\n",
    "    i += 1\n",
    "    if row['f_missed_block'] != 0:\n",
    "        df_consecutive_stats.loc[index, 'f_pool_missed'] = \"missed\"\n",
    "\n",
    "# we may use f_pool for scheduled blocks or f_pool_missed for proposed blocks\n",
    "# we want to have an incremental number that repeats with consecutive pools\n",
    "# the dataset is order by f_proposer_slot ascending\n",
    "df_consecutive_stats['consecutive'] = ((df_consecutive_stats.f_pool != df_consecutive_stats.f_pool.shift()).cumsum())\n",
    "df_consecutive_stats.to_csv('csv/proposers_metrics/pool_scheduled.csv')  \n",
    "df_consecutive = df_consecutive_stats[['f_pool', 'f_pool_missed', 'consecutive']] # select only these columns\n",
    "\n",
    "# count for every time there was a consecutive pool, how many consecutives\n",
    "df_consecutive_count = df_consecutive_stats[['consecutive', 'f_val_idx']].groupby(['consecutive']).count()\n",
    "df_consecutive_count = df_consecutive_count.rename(columns={\"f_val_idx\": \"consecutive_count\"}) # we call the count column \"consecutive_count\"\n",
    "# we join this using the consecutive column, which identifies every consecutive case individually\n",
    "df_consecutive_count = pd.merge(df_consecutive, df_consecutive_count, on=['consecutive']) \n",
    "# as the blocks are consecutive and we have the count, remove the extra lines\n",
    "df_consecutive_count = df_consecutive_count.drop_duplicates(subset='consecutive', keep=\"first\")\n",
    "df_consecutive_count = df_consecutive_count.rename(columns={\"consecutive\": \"number\"})\n",
    "df_consecutive_count = df_consecutive_count[['f_pool', 'consecutive_count', 'number']]\n",
    "# count how many times a pool got 5 consecutive scheduled blocks. If we use f_pool_missed it would be proposed blocks\n",
    "df_pool_consecutive = df_consecutive_count.groupby(['f_pool', 'consecutive_count']).count() \n",
    "df_consecutive_count = df_consecutive_count.rename(columns={\"number\": \"number_times\"})\n",
    "\n",
    "# Export to csv\n",
    "df_pool_consecutive.to_csv('csv/proposers_metrics/consecutive_pool_scheduled.csv')  \n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
