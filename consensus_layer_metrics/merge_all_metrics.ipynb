{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "\"\"\"\n",
    "This script obtains the following metrics per pool\n",
    "# Obtain an ordered set of metrics\n",
    "df_stats = df_stats[[\n",
    "    'f_merge', \n",
    "    'f_pool_name',\n",
    "    'number_validators',\n",
    "    'total_missed_source',\n",
    "    'total_missed_target',\n",
    "    'total_missed_head',\n",
    "    'expected_attestations',\n",
    "    'total_avg_reward',\n",
    "    'total_avg_max_reward',\n",
    "    'total_reward',\n",
    "    'missed_blocks',\n",
    "    'scheduled_blocks',\n",
    "    'status_active',\n",
    "    'status_exit',\n",
    "    'status_in queue to activation',\n",
    "    'status_slashed',\n",
    "    '%_missed_source',\n",
    "    '%_missed_target',\n",
    "    '%_missed_head',\n",
    "    '%_missed_blocks',\n",
    "    'total_max_reward']]\n",
    "\"\"\"\n",
    "\n",
    "# Load the database connection\n",
    "# %load_ext sql\n",
    "engine = sa.create_engine('postgresql://usr:pwd@localhost:5432/database')\n",
    "\n",
    "%reload_ext sql\n",
    "\n",
    "%sql $engine.url\n",
    "\n",
    "\n",
    "merge_epoch = 146875\n",
    "\n",
    "\n",
    "# 1. From here we obtain pools\n",
    "sql_stmnt = f\"\"\"\n",
    "    select \n",
    "    distinct(f_pool_name)\n",
    "    from eth2_pubkeys \n",
    "    group by f_pool_name\n",
    "\"\"\"\n",
    "\n",
    "df_pools = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# 2. Obtain missed blocks in string format \"[x, y, z]\"\n",
    "sql_stmnt = f\"\"\"\n",
    "    select \n",
    "    f_missed_blocks, \n",
    "    CASE \n",
    "    WHEN f_epoch <= {merge_epoch} THEN 'pre-merge' \n",
    "    WHEN f_epoch > {merge_epoch} THEN 'post-merge' \n",
    "    ELSE NULL END AS f_merge \n",
    "    from t_epoch_metrics_summary \n",
    "\"\"\"\n",
    "\n",
    "df_missed_blocks = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "missed_blocks = {\n",
    "    'pre-merge': 0,\n",
    "    'post-merge': 0\n",
    "}\n",
    "\n",
    "# Parse missed blocks from string array to int\n",
    "missed_blocks_list = []\n",
    "for i, row in df_missed_blocks.iterrows():\n",
    "    for j in str(row['f_missed_blocks']).split(\",\"):\n",
    "        text = j.replace(\"[\", \"\")\n",
    "        if text.isdigit():\n",
    "            missed_blocks[row['f_merge']] = missed_blocks[row['f_merge']] + 1\n",
    "            missed_blocks_list.append(int(text))\n",
    "\n",
    "\n",
    "# 3. mview_proposers contains all lines from validators proposing per slot: get missed blocks\n",
    "sql_stmnt = f\"\"\"\n",
    "    select \n",
    "        f_pool_name,\n",
    "        count(*) as missed_blocks,\n",
    "        CASE \n",
    "        WHEN f_proposer_slot/32 <= {merge_epoch} THEN 'pre-merge' \n",
    "        WHEN f_proposer_slot/32 > {merge_epoch} THEN 'post-merge' \n",
    "        ELSE NULL END AS f_merge \n",
    "    from t_proposer_duties\n",
    "    inner join eth2_pubkeys\n",
    "    on t_proposer_duties.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    where f_proposer_slot in ({\", \".join(str(x) for x in missed_blocks_list)})\n",
    "    group by f_pool_name, f_merge\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df_missed_blocks_pool = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# 3. mview_proposers contains all lines from validators proposing per slot: get scheduled blocks\n",
    "sql_stmnt = f\"\"\"\n",
    "    select \n",
    "        f_pool_name,\n",
    "        count(*) as scheduled_blocks,\n",
    "        CASE \n",
    "            WHEN f_proposer_slot/32 <= {merge_epoch} THEN 'pre-merge' \n",
    "            WHEN f_proposer_slot/32 > {merge_epoch} THEN 'post-merge' \n",
    "        ELSE NULL END AS f_merge \n",
    "    from t_proposer_duties\n",
    "    inner join eth2_pubkeys\n",
    "    on t_proposer_duties.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    group by f_pool_name, f_merge\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df_scheduled_blocks_pool = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# 5. mview_metrics_val_flags contains validators individual statistics for pre and post merge regarding attestation flags\n",
    "# Obtain missed flags for pre and post merge per pool\n",
    "sql_stmnt = f\"\"\"\n",
    "    select\n",
    "        f_merge,\n",
    "        f_pool_name,\n",
    "        sum(total_missed_source) as total_missed_source, \n",
    "        sum(total_missed_target) as total_missed_target, \n",
    "        sum(total_missed_head) as total_missed_head, \n",
    "        sum(number_lines) as expected_attestations\n",
    "    from mview_metrics_val_flags\n",
    "    inner join eth2_pubkeys\n",
    "    on mview_metrics_val_flags.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    group by f_merge, f_pool_name\n",
    "    order by f_merge desc;\n",
    "\"\"\"\n",
    "\n",
    "df_missed_flags = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "\n",
    "# 6. mview_metrics_val_rewards contains validators individual sum of rewards statistics for pre and post merge\n",
    "# Obtain the total reward and total max reward for pre and post merge per pool\n",
    "sql_stmnt = f\"\"\"\n",
    "    select\n",
    "        f_merge,\n",
    "        f_pool_name,\n",
    "        sum(total_reward) as total_reward, \n",
    "        sum(total_max_reward) as total_max_reward,\n",
    "        count(distinct(mview_metrics_val_rewards.f_val_idx)) as number_validators\n",
    "    from mview_metrics_val_rewards\n",
    "    inner join eth2_pubkeys\n",
    "    on mview_metrics_val_rewards.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    group by f_merge, f_pool_name\n",
    "    order by f_merge desc;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "df_rewards = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "# 7. mview_metrics_val_rewards contains validators individual sum of rewards statistics for pre and post merge\n",
    "# Obtain the average reward and total max reward for pre and post merge per pool\n",
    "sql_stmnt = f\"\"\"\n",
    "    select\n",
    "        f_merge,\n",
    "        f_pool_name,\n",
    "        avg(f_avg_reward) as total_avg_reward, \n",
    "        avg(f_avg_max_reward) as total_avg_max_reward\n",
    "    from mview_val_avg\n",
    "    inner join eth2_pubkeys\n",
    "    on mview_val_avg.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    group by f_merge, f_pool_name \n",
    "    order by f_merge desc;\n",
    "\"\"\"\n",
    "\n",
    "df_avg_rewards = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "\n",
    "\n",
    "# 8. mview_metrics_val_status contains data about which status each validator has been in pre and post merge\n",
    "# Obtain the count of status per pool\n",
    "sql_stmnt = f\"\"\"\n",
    "    select \n",
    "    f_merge, \n",
    "    f_pool_name, \n",
    "    f_status, \n",
    "    sum(f_count) as f_count\n",
    "    from mview_metrics_val_status\n",
    "    inner join eth2_pubkeys\n",
    "    on mview_metrics_val_status.f_val_idx = eth2_pubkeys.f_val_idx\n",
    "    group by f_pool_name, f_status, f_merge\n",
    "    order by f_status\n",
    "\"\"\"\n",
    "\n",
    "df_status = pd.read_sql(sql_stmnt, engine)\n",
    "print(\"Executed\")\n",
    "\n",
    "\n",
    "# --------------------- Start the analysis -------------------------\n",
    "\n",
    "# Get all the previous data and merge into a single dataframe\n",
    "# We always merge the datasets using the \"f_pool_name\" as all data was extracted using this same column\n",
    "\n",
    "df_stats = pd.merge(df_pools, df_missed_flags, on=['f_pool_name'], how='left').fillna(0)\n",
    "df_stats = pd.merge(df_stats, df_missed_blocks_pool, on=['f_pool_name', 'f_merge'], how='left').fillna(0)\n",
    "df_stats = pd.merge(df_stats, df_scheduled_blocks_pool, on=['f_pool_name', 'f_merge'], how='left').fillna(0)\n",
    "df_stats = pd.merge(df_stats, df_rewards, on=['f_pool_name', 'f_merge'], how='left').fillna(0)\n",
    "df_stats = pd.merge(df_stats, df_avg_rewards, on=['f_pool_name', 'f_merge'], how='left').fillna(0)\n",
    "\n",
    "# In the case of status we want to separate each status into a single column so we dont get repeated rows with same pool\n",
    "df_status = df_status.pivot(index=['f_merge', 'f_pool_name'], columns='f_status', values='f_count').add_prefix(\"status_\").reset_index()\n",
    "\n",
    "df_stats = pd.merge(df_stats, df_status, on=['f_merge', 'f_pool_name'], how='left').fillna(0)\n",
    "\n",
    "# Finally, build some statistics based on the previous merged dataset\n",
    "df_stats['%_missed_source'] = df_stats['total_missed_source']/(df_stats['expected_attestations']) * 100\n",
    "df_stats['%_missed_target'] = df_stats['total_missed_target']/(df_stats['expected_attestations']) * 100\n",
    "df_stats['%_missed_head'] = df_stats['total_missed_head']/(df_stats['expected_attestations']) * 100\n",
    "df_stats['%_missed_blocks'] = df_stats['missed_blocks']/(df_stats['scheduled_blocks']) * 100\n",
    "\n",
    "\n",
    "# Obtain an ordered set of metrics\n",
    "df_stats = df_stats[[\n",
    "    'f_merge', \n",
    "    'f_pool_name',\n",
    "    'number_validators',\n",
    "    'total_missed_source',\n",
    "    'total_missed_target',\n",
    "    'total_missed_head',\n",
    "    'expected_attestations',\n",
    "    'total_avg_reward',\n",
    "    'total_avg_max_reward',\n",
    "    'total_reward',\n",
    "    'missed_blocks',\n",
    "    'scheduled_blocks',\n",
    "    'status_active',\n",
    "    'status_exit',\n",
    "    'status_in queue to activation',\n",
    "    'status_slashed',\n",
    "    '%_missed_source',\n",
    "    '%_missed_target',\n",
    "    '%_missed_head',\n",
    "    '%_missed_blocks',\n",
    "    'total_max_reward']]\n",
    "df_stats = df_stats.set_index(['f_merge', 'f_pool_name'])\n",
    "\n",
    "# Export to csv\n",
    "df_stats.to_csv('csv/all_metrics/out_merge_metrics.csv')  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
