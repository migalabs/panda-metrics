{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# %load_ext sql # only the first time\n",
    "engine = sa.create_engine('postgresql://rewards_reader:==Hji52.(M4@localhost:5433/beaconchain_latest')\n",
    "\n",
    "# Variables\n",
    "MERGE_EPOCH = 146875\n",
    "\n",
    "# CSV names:\n",
    "REWARDS_POOL_EPOCH = 'rewards_pool_epoch.csv'\n",
    "VALIDATORS_PER_EPOCH = 'vals_per_epoch.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the a CSV file with the distribution of Rewards, Max Reward per epoch and per Pool\n",
    "# NOTE: this might take up to 20-30mins depending on your machine and internet connection\n",
    "\n",
    "# SQL QUERY:\n",
    "sql_query = \"\"\"\n",
    "    SELECT \n",
    "\trewds.f_epoch, \n",
    "\tsum(rewds.f_reward) as reward,\n",
    "\tsum(rewds.f_max_reward) as max_reward, \n",
    "\tsum(CASE WHEN rewds.f_missing_source THEN 1 ELSE 0 END) as m_source, \n",
    "\tsum(CASE WHEN rewds.f_missing_target THEN 1 ELSE 0 END) as m_target, \n",
    "\tsum(CASE WHEN rewds.f_missing_head THEN 1 ELSE 0 END) as m_head,\n",
    "\trewds.f_pool\n",
    "    FROM (\n",
    "        SELECT \n",
    "            f_epoch, \n",
    "            t_validator_rewards_summary.f_val_idx as f_val_idx, \n",
    "            f_reward, \n",
    "            f_max_reward, \n",
    "\t\t\tf_missing_source,\n",
    "\t\t\tf_missing_target,\n",
    "\t\t\tf_missing_head,\n",
    "        \tCASE \n",
    "\t\t\t\tWHEN f_pool IS NULL\n",
    "\t\t\t\t\tTHEN 'others'\n",
    "\t\t\t\tELSE f_pool\n",
    "\t\t\t\tEND AS f_pool    \n",
    "        FROM t_validator_rewards_summary \n",
    "        LEFT JOIN eth2_pubkeys ON t_validator_rewards_summary.f_val_idx=eth2_pubkeys.f_val_idx\n",
    "    ) as rewds\n",
    "    GROUP BY f_epoch, f_pool\n",
    "    ORDER BY f_epoch ASC;\n",
    "\"\"\"\n",
    "\n",
    "p = pd.read_sql_query(sql_query, engine)\n",
    "p.to_csv(REWARDS_POOL_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Only a column name can be used for the key in a dtype mappings argument. 'perc' not found in columns.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m p \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(REWARDS_POOL_EPOCH)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Since we want to display CL/Attetation-related rewards - remove slashings and block proposals \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m p \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mastype({\u001b[39m'\u001b[39;49m\u001b[39mperc\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mfloat\u001b[39;49m})\n\u001b[1;32m      6\u001b[0m p \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mdrop(p[p\u001b[39m.\u001b[39mperc \u001b[39m>\u001b[39m \u001b[39m100\u001b[39m]\u001b[39m.\u001b[39mindex)\n\u001b[1;32m      7\u001b[0m p \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mdrop(p[p\u001b[39m.\u001b[39mperc \u001b[39m<\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m]\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/devel/python_venvs/plotter/lib/python3.8/site-packages/pandas/core/generic.py:6212\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6210\u001b[0m \u001b[39mfor\u001b[39;00m col_name \u001b[39min\u001b[39;00m dtype_ser\u001b[39m.\u001b[39mindex:\n\u001b[1;32m   6211\u001b[0m     \u001b[39mif\u001b[39;00m col_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m-> 6212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m   6213\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mOnly a column name can be used for the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   6214\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mkey in a dtype mappings argument. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   6215\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcol_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not found in columns.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   6216\u001b[0m         )\n\u001b[1;32m   6218\u001b[0m dtype_ser \u001b[39m=\u001b[39m dtype_ser\u001b[39m.\u001b[39mreindex(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, fill_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   6220\u001b[0m results \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Only a column name can be used for the key in a dtype mappings argument. 'perc' not found in columns.\""
     ]
    }
   ],
   "source": [
    "# Display the percentage of reward that each pool got out of the max they could\n",
    "p = pd.read_csv(REWARDS_POOL_EPOCH)\n",
    "\n",
    "# Since we want to display CL/Attetation-related rewards - remove slashings and block proposals \n",
    "p['perc'] = (p['reward'] / p['max_reward']) * 100\n",
    "p = p.astype({'perc': float})\n",
    "p = p.drop(p[p.perc > 100].index)\n",
    "p = p.drop(p[p.perc < -100].index)\n",
    "\n",
    "# Get the aggregation of all the pools and get the rewards/max_reward per pool\n",
    "p = p.groupby(['f_epoch']).sum()\n",
    "p['perc_2'] = (p['reward'] / p['max_reward']) * 100\n",
    "p = p.astype({'perc_2': float})\n",
    "\n",
    "# roll the large dataset into averages of 8 sample windows\n",
    "p_mean = p['perc_2'].rolling(8, axis=0).mean()\n",
    "\n",
    "# Display the data that we just prepared\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "ax = plt.plot(p_mean)\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"extracted reward (%)\")\n",
    "plt.title(\"reward / max extractable reward\")\n",
    "\n",
    "# Mark the Merge Epoch\n",
    "plt.vlines(x=MERGE_EPOCH, ymin=50, ymax=100 , color='r')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the distribution of total active validators over the total of epochs\n",
    "\n",
    "sql_query = \"\"\"\n",
    "    select \n",
    "        f_epoch,\n",
    "        count(f_val_idx) as tot_val\n",
    "    from t_validator_rewards_summary\n",
    "    group by f_epoch\n",
    "    order by f_epoch ASC;\n",
    "\"\"\"\n",
    "\n",
    "p = pd.read_sql_query(sql_query, engine)\n",
    "p['perc'] = (p['reward'] / p['max_reward']) * 100\n",
    "p.to_csv(VALIDATORS_PER_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of missed attestation flags per total active validators\n",
    "\n",
    "# Read both CSVs in different panda OBJs\n",
    "o = pd.read_csv(VALIDATORS_PER_EPOCH)\n",
    "p = pd.read_csv(REWARDS_POOL_EPOCH)\n",
    "\n",
    "# Get the aggregation of missed flags for each epoch\n",
    "p = p.groupby(['f_epoch']).sum()\n",
    "p['tot_m_flags'] = p['m_source'] + p['m_target'] + p['m_head']\n",
    "\n",
    "# Aggregate the mean of the distribution in windows of 8 samples\n",
    "p = p.rolling(8, axis=0).mean()\n",
    "\n",
    "# Merge both datasets based on epoch to get the ratio of missed flags\n",
    "p = pd.merge(p, o, on='f_epoch')\n",
    "\n",
    "# Calculate the ratio of missed per each flag (%)\n",
    "p['m_source_r'] = (p['m_source'] / p['tot_val']) * 100\n",
    "p['m_target_r'] = (p['m_target'] / p['tot_val']) * 100\n",
    "p['m_head_r'] = (p['m_head']/ p['tot_val'])  * 100\n",
    "p['tot_m_r'] = ((p['tot_m_flags'])/ p['tot_val'])  * 100\n",
    "\n",
    "# Display the distributions\n",
    "sns.set_context(\"talk\", font_scale=1.1)\n",
    "\n",
    "f, axs = plt.subplots(2,1,\n",
    "                    figsize=(15,12),\n",
    "                    sharex=True)\n",
    "\n",
    "sns.lineplot(ax=axs[0], x='f_epoch', y='tot_m_r', data=p, label='Total Missed', color='grey')\n",
    "\n",
    "sns.lineplot(ax=axs[1], x='f_epoch', y='m_source_r', data=p, label='Missed Source')\n",
    "sns.lineplot(ax=axs[1], x='f_epoch', y='m_target_r', data=p, label='Missed Target')\n",
    "sns.lineplot(ax=axs[1], x='f_epoch', y='m_head_r', data=p, label='Missed Heads')\n",
    "\n",
    "\n",
    "axs[1].set(xlabel=\"Epoch\", ylabel=\"Ratio of Missed Flags\")\n",
    "axs[0].set(ylabel=\"Ratio of Missed Flags\")\n",
    "\n",
    "axs[0].axvline(x=146875, ymin=0, ymax=55000, color='r')\n",
    "axs[1].axvline(x=146875, ymin=0, ymax=25000, color='r')\n",
    "\n",
    "axs[0].grid(axis='both')\n",
    "axs[1].grid(axis='both')\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('plotter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27c6d93b683c7a1975bfd893e997da1d087883bf6b96d34d1e63ecc137ac54d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
